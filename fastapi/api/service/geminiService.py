from functools import lru_cache
from typing import AsyncGenerator, Optional
from langchain_openai import ChatOpenAI
from langchain_core.messages import HumanMessage, SystemMessage, AIMessage
from langchain_classic.agents import AgentExecutor, create_react_agent
from fastapi import Depends
from sqlmodel import Session
from api.mapper import (
    AiHistoryMapper, 
    get_ai_history_mapper, 
    UserMapper, 
    get_user_mapper
)
from .baseAIService import (
    BaseAiService, 
    get_agent_prompt, 
    initialize_ai_tools
)
from common.agent import IntentRouter, UserPermissionManager
from common.utils import fileLogger as logger
from config import load_config, load_secret_config

class GeminiService(BaseAiService):
    """Gemini AI Service"""
    
    
    def __init__(
            self, 
            ai_history_mapper: AiHistoryMapper = None, 
            user_mapper: Optional[UserMapper] = None
        ):
        super().__init__(ai_history_mapper, service_name="Gemini")
        
        # åˆå§‹åŒ–æƒé™ç®¡ç†å™¨
        self.perm_manager = UserPermissionManager(user_mapper)
        
        # åˆå§‹åŒ–Geminiå®¢æˆ·ç«¯ï¼ˆä½¿ç”¨ç¬¬ä¸‰æ–¹ä»£ç†ï¼‰
        try:
            gemini_cfg = load_config("gemini") or {}
            gemini_secret = load_secret_config("gemini") or {}
            self._api_key: str = gemini_secret.get("api_key")
            self._model_name: str = gemini_cfg.get("model_name", "gemini-2.0-flash")
            self._base_url: str = gemini_cfg.get("base_url", "https://api.openai-proxy.org/google")
            self._timeout: int = gemini_cfg.get("timeout", 30)
            
            if self._api_key:
                # ä½¿ç”¨ ChatOpenAI è°ƒç”¨ç¬¬ä¸‰æ–¹ä»£ç†çš„ Gemini API
                self.llm = ChatOpenAI(
                    model=self._model_name,
                    api_key=self._api_key,
                    base_url=self._base_url,
                    temperature=0.7,
                    timeout=self._timeout,
                )
                
                # åˆå§‹åŒ–å·¥å…·
                try:
                    _, _, _, self.all_tools = initialize_ai_tools()
                    
                    # åˆå§‹åŒ–æ„å›¾è·¯ç”±å™¨ï¼ˆä¼ é€’ user_mapperï¼‰
                    self.intent_router = IntentRouter(self.llm, user_mapper=user_mapper)
                    
                    # åˆ›å»ºReAct Agent
                    agent_prompt = get_agent_prompt()
                    self.agent = create_react_agent(
                        llm=self.llm,
                        tools=self.all_tools,
                        prompt=agent_prompt
                    )
                    
                    self.agent_executor = AgentExecutor(
                        agent=self.agent,
                        tools=self.all_tools,
                        verbose=True,
                        handle_parsing_errors=True,
                        max_iterations=5,
                        return_intermediate_steps=True
                    )
                    
                    logger.info("Gemini AgentæœåŠ¡åˆå§‹åŒ–å®Œæˆ")
                except Exception as tool_error:
                    logger.warning(f"å·¥å…·åˆå§‹åŒ–éƒ¨åˆ†å¤±è´¥: {tool_error}, é™çº§ä¸ºåŸºç¡€å¯¹è¯æ¨¡å¼")
                    self.agent_executor = None
                    self.intent_router = None
            else:
                self.llm = None
                self.agent_executor = None
                self.intent_router = None
                logger.warning("Geminié…ç½®ä¸å®Œæ•´ï¼Œå®¢æˆ·ç«¯æœªåˆå§‹åŒ–")
        except Exception as e:
            self.llm = None
            self.agent_executor = None
            self.intent_router = None
            logger.error(f"åˆå§‹åŒ–Geminiå®¢æˆ·ç«¯å¤±è´¥: {e}")
    
    async def basic_chat(self, message: str) -> str:
        """æœ€åŸºç¡€çš„å¯¹è¯æ¥å£ - ä¸ä½¿ç”¨çŸ¥è¯†åº“å’Œå‘é‡æ•°æ®åº“"""
        try:
            logger.info(f"åŸºç¡€å¯¹è¯: {message}")
            
            if not getattr(self, 'llm', None):
                return "èŠå¤©æœåŠ¡æœªé…ç½®æˆ–åˆå§‹åŒ–å¤±è´¥"
            
            # ä½¿ç”¨ LangChain è°ƒç”¨
            messages = [
                SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸­æ–‡AIåŠ©æ‰‹ï¼Œç”¨äºæä¾›æ–‡ç« å’Œåšå®¢æ¨èåŠåˆ†æç³»ç»Ÿæ•°æ®ã€‚"),
                HumanMessage(content=message)
            ]
            response = await self.llm.ainvoke(messages)
            
            result: str = response.content
            logger.info(f"Gemini åŸºç¡€å›å¤é•¿åº¦: {len(result)} å­—ç¬¦")
            return result
                
        except Exception as e:
            logger.error(f"Gemini åŸºç¡€å¯¹è¯å¼‚å¸¸: {str(e)}")
            return f"å¯¹è¯æœåŠ¡å¼‚å¸¸: {str(e)}"

    async def with_reference_chat(self, message: str, reference_content: str) -> str:
        """
        åŸºäºå‚è€ƒæ–‡æœ¬çš„å¯¹è¯æ¥å£ - ä½¿ç”¨æƒå¨å‚è€ƒæ–‡æœ¬è¿›è¡Œè¯„è®ºå’Œæ‰“åˆ†
        
        Args:
            message: ç”¨æˆ·çš„æ¶ˆæ¯æˆ–è¦è¯„ä»·çš„æ–‡ç« å†…å®¹
            reference_content: æƒå¨å‚è€ƒæ–‡æœ¬å†…å®¹
            
        Returns:
            è¯„ä»·å’Œæ‰“åˆ†ç»“æœ
        """
        try:
            logger.info(f"åŸºäºå‚è€ƒæ–‡æœ¬çš„å¯¹è¯ï¼ˆé•¿åº¦: {len(reference_content)}ï¼‰")
            
            if not getattr(self, 'llm', None):
                return "èŠå¤©æœåŠ¡æœªé…ç½®æˆ–åˆå§‹åŒ–å¤±è´¥"
            
            # ä½¿ç”¨åŸºç±»çš„æç¤ºè¯æ–¹æ³•
            prompt = self._get_reference_evaluation_prompt(message, reference_content)
            
            # ä½¿ç”¨ LangChain è°ƒç”¨
            messages = [
                SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„æ–‡ç« è¯„ä»·åŠ©æ‰‹ã€‚è¯·æ ¹æ®æä¾›çš„æƒå¨å‚è€ƒæ–‡æœ¬è¿›è¡Œå®¢è§‚ã€ä¸“ä¸šçš„è¯„ä»·ã€‚"),
                HumanMessage(content=prompt)
            ]
            
            response = await self.llm.ainvoke(messages)
            result: str = response.content
            logger.info(f"Gemini å‚è€ƒæ–‡æœ¬å¯¹è¯å›å¤é•¿åº¦: {len(result)} å­—ç¬¦")
            return result
            
        except Exception as e:
            logger.error(f"Geminiå‚è€ƒæ–‡æœ¬å¯¹è¯å¼‚å¸¸: {str(e)}")
            return f"å¯¹è¯æœåŠ¡å¼‚å¸¸: {str(e)}"

    async def summarize_content(self, content: str, max_length: int = 1000) -> str:
        """
        ä½¿ç”¨Geminiå¤§æ¨¡å‹å¯¹æå–çš„å†…å®¹è¿›è¡Œæ€»ç»“
        
        Args:
            content: éœ€è¦æ€»ç»“çš„å†…å®¹
            max_length: æ€»ç»“åçš„æœ€å¤§é•¿åº¦
            
        Returns:
            æ€»ç»“åçš„å†…å®¹
        """
        try:
            logger.info(f"Geminiå¼€å§‹æ€»ç»“å†…å®¹ï¼ŒåŸå§‹é•¿åº¦: {len(content)} å­—ç¬¦")
            
            if not getattr(self, 'llm', None):
                return "æ€»ç»“æœåŠ¡æœªé…ç½®æˆ–åˆå§‹åŒ–å¤±è´¥"
            
            # ä½¿ç”¨åŸºç±»çš„æç¤ºè¯æ–¹æ³•
            prompt = self._get_summarize_prompt(content, max_length)
            
            messages = [
                SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸“ä¸šçš„å†…å®¹æ€»ç»“åŠ©æ‰‹ã€‚è¯·ç²¾å‡†æå–æ ¸å¿ƒä¿¡æ¯ï¼Œç”¨å‡ç»ƒçš„è¯­è¨€è¿›è¡Œæ€»ç»“ã€‚"),
                HumanMessage(content=prompt)
            ]
            
            response = await self.llm.ainvoke(messages)
            result: str = response.content[:max_length]
            logger.info(f"Geminiå†…å®¹æ€»ç»“å®Œæˆï¼Œæ€»ç»“é•¿åº¦: {len(result)} å­—ç¬¦")
            return result
            
        except Exception as e:
            logger.error(f"Geminiå†…å®¹æ€»ç»“å¼‚å¸¸: {str(e)}")
            return f"æ€»ç»“æœåŠ¡å¼‚å¸¸: {str(e)}"

    async def simple_chat(self, message: str, user_id: int = 0, db: Optional[Session] = None) -> str:
        """å¢å¼ºçš„èŠå¤©æ¥å£ - ä½¿ç”¨Agentå’ŒRAG"""
        try:
            logger.info(f"ç”¨æˆ· {user_id} å‘é€æ¶ˆæ¯: {message}")
            
            if not getattr(self, 'llm', None):
                return "èŠå¤©æœåŠ¡æœªé…ç½®æˆ–åˆå§‹åŒ–å¤±è´¥"
            
            # å¦‚æœAgentæœªåˆå§‹åŒ–ï¼Œé™çº§ä¸ºåŸºç¡€å¯¹è¯
            if not self.agent_executor:
                return await self.basic_chat(message)
            
            # 1. æƒé™æ£€æŸ¥ï¼ˆå¦‚æœæœ‰ç”¨æˆ·IDå’Œæ•°æ®åº“ä¼šè¯ï¼‰
            intent = "general_chat"
            if self.intent_router and db and user_id:
                intent, has_permission, permission_msg = self.intent_router.route_with_permission_check(message, user_id, db)
                logger.info(f"è¯†åˆ«æ„å›¾: {intent}, æœ‰æƒé™: {has_permission}")
                
                # å¦‚æœæ²¡æœ‰æƒé™ï¼Œç›´æ¥è¿”å›æƒé™æç¤ºä¿¡æ¯
                if not has_permission:
                    logger.info(f"ç”¨æˆ· {user_id} æ— æƒé™è®¿é—®: {intent}")
                    return permission_msg or "æ‚¨æ²¡æœ‰æƒé™è®¿é—®æ­¤åŠŸèƒ½ï¼Œè¯·è”ç³»ç®¡ç†å‘˜å¼€é€šç›¸å…³æƒé™ã€‚"
                    
            elif self.intent_router:
                intent = self.intent_router.route(message)
                logger.info(f"è¯†åˆ«æ„å›¾: {intent}")
            
            # 2. åŠ è½½èŠå¤©å†å²
            chat_history = []
            if db and user_id:
                chat_history = self._load_chat_history(user_id, db)
            
            # 3. æ ¹æ®æ„å›¾é€‰æ‹©å¤„ç†æ–¹å¼
            if intent == "general_chat":
                # ç®€å•é—²èŠï¼Œç›´æ¥å›å¤
                history_messages = []
                for human_msg, ai_msg in chat_history:
                    history_messages.append(HumanMessage(content=human_msg))
                    history_messages.append(AIMessage(content=ai_msg))
                
                messages = [
                    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„ä¸­æ–‡AIåŠ©æ‰‹ã€‚"),
                    *history_messages,
                    HumanMessage(content=message)
                ]
                response = await self.llm.ainvoke(messages)
                result = response.content
                
            else:
                # ä½¿ç”¨Agentå¤„ç†ï¼Œè®©Agentè‡ªå·±å†³å®šä½¿ç”¨å“ªäº›å·¥å…·
                # Agentå¯ä»¥åŒæ—¶è°ƒç”¨SQLå·¥å…·å’ŒRAGå·¥å…·ï¼Œæˆ–åªè°ƒç”¨å…¶ä¸­ä¸€ä¸ª
                logger.info("ä½¿ç”¨Agentå¤„ç†ï¼Œå¯åŒæ—¶è°ƒç”¨SQLå’ŒRAGå·¥å…·")
                
                # æ·»åŠ å†å²ä¸Šä¸‹æ–‡åˆ°è¾“å…¥
                context = self._build_chat_context(chat_history)
                full_input = context + f"å½“å‰é—®é¢˜: {message}"
                
                agent_response = await self.agent_executor.ainvoke({"input": full_input})
                result = agent_response.get("output", "æ— æ³•è·å–ç»“æœ")
            
            logger.info(f"Geminiå›å¤é•¿åº¦: {len(result)} å­—ç¬¦")
            return result
                
        except Exception as e:
            logger.error(f"GeminièŠå¤©å¼‚å¸¸: {str(e)}")
            if "API_KEY_INVALID" in str(e) or "invalid API key" in str(e):
                return "APIå¯†é’¥æ— æ•ˆã€‚è¯·æ£€æŸ¥Gemini APIå¯†é’¥é…ç½®ã€‚"
            elif "QUOTA_EXCEEDED" in str(e):
                return "APIé…é¢å·²è¶…é™ã€‚è¯·ç¨åé‡è¯•æˆ–æ£€æŸ¥é…é¢è®¾ç½®ã€‚"
            elif "RATE_LIMIT_EXCEEDED" in str(e):
                return "APIè°ƒç”¨é¢‘ç‡è¶…é™ã€‚è¯·ç¨åé‡è¯•ã€‚"
            return f"èŠå¤©æœåŠ¡å¼‚å¸¸: {str(e)}"
        
    async def stream_chat(self, message: str, user_id: int = 0, db: Optional[Session] = None) -> AsyncGenerator[str, None]:
        """æµå¼èŠå¤©æ¥å£
        
        Yields:
            dict æ ¼å¼: {"type": "tool_call|thinking|content", "content": "..."}
            æˆ–ç›´æ¥è¿”å›å­—ç¬¦ä¸²ï¼ˆå‘åå…¼å®¹ï¼‰
        """
        try:
            logger.info(f"ç”¨æˆ· {user_id} å¼€å§‹æµå¼èŠå¤©: {message}")
            
            if not getattr(self, 'llm', None):
                yield {"type": "error", "content": "èŠå¤©æœåŠ¡æœªé…ç½®æˆ–åˆå§‹åŒ–å¤±è´¥"}
                return
            
            # å¦‚æœAgentæœªåˆå§‹åŒ–ï¼Œé™çº§ä¸ºåŸºç¡€æµå¼å¯¹è¯
            if not self.agent_executor:
                # åŠ è½½èŠå¤©å†å²
                chat_history = []
                if db and user_id:
                    chat_history = self._load_chat_history(user_id, db)
                
                # æ„å»ºæ¶ˆæ¯
                history_messages = []
                for human_msg, ai_msg in chat_history:
                    history_messages.append(HumanMessage(content=human_msg))
                    history_messages.append(AIMessage(content=ai_msg))
                
                messages = [
                    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªä¸­æ–‡AIåŠ©æ‰‹ï¼Œç”¨äºæä¾›æ–‡ç« å’Œåšå®¢æ¨èåŠåˆ†æç³»ç»Ÿæ•°æ®ã€‚"),
                    *history_messages,
                    HumanMessage(content=message)
                ]
                
                try:
                    async for chunk in self.llm.astream(messages):
                        try:
                            if chunk.content:
                                logger.debug(f"æ”¶åˆ°æµå¼å†…å®¹å—ï¼Œé•¿åº¦: {len(chunk.content)} å­—ç¬¦")
                                yield {"type": "content", "content": chunk.content}
                        except Exception as chunk_error:
                            logger.error(f"å¤„ç†æµå¼å†…å®¹å—å¼‚å¸¸: {str(chunk_error)}")
                            continue
                except Exception as stream_error:
                    error_msg = str(stream_error)
                    logger.error(f"åŸºç¡€æµå¼å¯¹è¯å¤±è´¥: {error_msg}")
                    
                    # æ£€æµ‹é…é¢å’Œé™æµé”™è¯¯ï¼Œä½œä¸ºæ™®é€šå†…å®¹è¿”å›
                    if "ResourceExhausted" in error_msg or "429" in error_msg:
                        if "quota" in error_msg.lower():
                            yield {"type": "content", "content": "ğŸ˜” Gemini API é…é¢å·²ç”¨å®Œã€‚å»ºè®®åˆ‡æ¢åˆ°è±†åŒ…æˆ–é€šä¹‰åƒé—®æœåŠ¡ã€‚"}
                        else:
                            yield {"type": "content", "content": "ğŸ˜” Gemini API è¯·æ±‚é¢‘ç‡è¶…é™ã€‚è¯·ç¨åå†è¯•ã€‚"}
                    elif "invalid" in error_msg.lower() and "key" in error_msg.lower():
                        yield {"type": "content", "content": "âŒ API å¯†é’¥æ— æ•ˆã€‚"}
                    else:
                        yield {"type": "content", "content": f"âŒ æœåŠ¡å¼‚å¸¸: {error_msg[:100]}"}
                return
            
            # 1. æƒé™æ£€æŸ¥ï¼ˆå¦‚æœæœ‰ç”¨æˆ·IDå’Œæ•°æ®åº“ä¼šè¯ï¼‰
            intent = "general_chat"
            permission_info = ""
            if self.intent_router and db and user_id:
                intent, has_permission, permission_msg = self.intent_router.route_with_permission_check(message, user_id, db)
                logger.info(f"è¯†åˆ«æ„å›¾: {intent}, æœ‰æƒé™: {has_permission}")
                
                # å¦‚æœæ²¡æœ‰æƒé™ï¼Œç›´æ¥æµå¼è¾“å‡ºæƒé™æç¤ºä¿¡æ¯å¹¶è¿”å›
                if not has_permission:
                    logger.info(f"ç”¨æˆ· {user_id} æ— æƒé™è®¿é—®: {intent}")
                    permission_message = permission_msg or "æ‚¨æ²¡æœ‰æƒé™è®¿é—®æ­¤åŠŸèƒ½ï¼Œè¯·è”ç³»ç®¡ç†å‘˜å¼€é€šç›¸å…³æƒé™ã€‚"
                    
                    # å‘é€æ€è€ƒè¿‡ç¨‹ï¼ˆè¯´æ˜ä¸ºä»€ä¹ˆæ²¡æœ‰æƒé™ï¼‰
                    thinking = f"æ£€æµ‹åˆ°ç”¨æˆ·è¯·æ±‚éœ€è¦ {intent} æƒé™ï¼Œä½†å½“å‰ç”¨æˆ·æƒé™ä¸è¶³ã€‚"
                    yield {"type": "thinking", "content": thinking}
                    
                    # æµå¼è¾“å‡ºå‹å¥½çš„æƒé™æç¤ºä¿¡æ¯
                    for char in permission_message:
                        yield {"type": "content", "content": char}
                    return
                    
            elif self.intent_router:
                intent = self.intent_router.route(message)
                logger.info(f"è¯†åˆ«æ„å›¾: {intent}")
            
            # 2. åŠ è½½èŠå¤©å†å²
            chat_history = []
            if db and user_id:
                chat_history = self._load_chat_history(user_id, db)
            
            # 3. æ ¹æ®æ„å›¾é€‰æ‹©å¤„ç†æ–¹å¼
            if intent == "general_chat":
                # ç®€å•é—²èŠï¼Œç›´æ¥æµå¼å›å¤
                history_messages = []
                for human_msg, ai_msg in chat_history:
                    history_messages.append(HumanMessage(content=human_msg))
                    history_messages.append(AIMessage(content=ai_msg))
                
                messages = [
                    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„ä¸­æ–‡AIåŠ©æ‰‹ã€‚"),
                    *history_messages,
                    HumanMessage(content=message)
                ]
                
                try:
                    async for chunk in self.llm.astream(messages):
                        try:
                            if chunk.content:
                                yield {"type": "content", "content": chunk.content}
                        except Exception as chunk_error:
                            logger.error(f"å¤„ç†æµå¼å†…å®¹å—å¼‚å¸¸: {str(chunk_error)}")
                            continue
                except Exception as stream_error:
                    error_msg = str(stream_error)
                    logger.error(f"æµå¼èŠå¤©å¤±è´¥: {error_msg}")
                    
                    # æ£€æµ‹é…é¢å’Œé™æµé”™è¯¯ï¼Œä½œä¸ºæ™®é€šå†…å®¹è¿”å›
                    if "ResourceExhausted" in error_msg or "429" in error_msg:
                        if "quota" in error_msg.lower():
                            yield {"type": "content", "content": "ğŸ˜” Gemini API é…é¢å·²ç”¨å®Œã€‚å»ºè®®åˆ‡æ¢åˆ°è±†åŒ…æˆ–é€šä¹‰åƒé—®æœåŠ¡ã€‚"}
                        else:
                            yield {"type": "content", "content": "ğŸ˜” Gemini API è¯·æ±‚é¢‘ç‡è¶…é™ã€‚è¯·ç¨åå†è¯•ã€‚"}
                    elif "invalid" in error_msg.lower() and "key" in error_msg.lower():
                        yield {"type": "content", "content": "âŒ API å¯†é’¥æ— æ•ˆã€‚"}
                    else:
                        yield {"type": "content", "content": f"âŒ æœåŠ¡å¼‚å¸¸: {error_msg[:100]}"}
                
            else:
                # ä½¿ç”¨Agentå¤„ç†è·å–ä¿¡æ¯,ç„¶åæµå¼è¾“å‡ºæœ€ç»ˆç­”æ¡ˆ
                logger.info("ä½¿ç”¨Agentå¤„ç†,å¯åŒæ—¶è°ƒç”¨SQLå’ŒRAGå·¥å…·")
                
                # æ·»åŠ å†å²ä¸Šä¸‹æ–‡åˆ°è¾“å…¥
                context = self._build_chat_context(chat_history)
                full_input = context + f"å½“å‰é—®é¢˜: {message}"
                
                # ç¬¬ä¸€æ­¥: ä½¿ç”¨Agentè·å–ä¿¡æ¯å’Œæ€è€ƒ
                logger.info("Agentå¼€å§‹å¤„ç†...")
                try:
                    agent_response = await self.agent_executor.ainvoke({"input": full_input})
                    agent_result = agent_response.get("output", "æ— æ³•è·å–ç»“æœ")
                except Exception as agent_error:
                    error_msg = str(agent_error)
                    logger.error(f"Agentæ‰§è¡Œå¤±è´¥: {error_msg}")
                    
                    # æ£€æµ‹é…é¢å’Œé™æµé”™è¯¯å¹¶ç›´æ¥è¿”å›ï¼Œä½œä¸ºæ™®é€šå†…å®¹
                    if "ResourceExhausted" in error_msg or "429" in error_msg:
                        if "quota" in error_msg.lower():
                            yield {"type": "content", "content": "ğŸ˜” Gemini API é…é¢å·²ç”¨å®Œã€‚å»ºè®®åˆ‡æ¢åˆ°è±†åŒ…æˆ–é€šä¹‰åƒé—®æœåŠ¡ã€‚"}
                        else:
                            yield {"type": "content", "content": "ğŸ˜” Gemini API è¯·æ±‚é¢‘ç‡è¶…é™ã€‚è¯·ç¨åå†è¯•æˆ–åˆ‡æ¢å…¶ä»–æœåŠ¡ã€‚"}
                    elif "API_KEY_INVALID" in error_msg or "invalid API key" in error_msg.lower():
                        yield {"type": "content", "content": "âŒ API å¯†é’¥æ— æ•ˆã€‚"}
                    else:
                        yield {"type": "content", "content": f"âŒ AI å¤„ç†å¤±è´¥: {error_msg[:100]}"}
                    return
                
                # æå–ä¸­é—´æ­¥éª¤ï¼ˆå·¥å…·è°ƒç”¨ï¼‰
                intermediate_steps = agent_response.get("intermediate_steps", [])
                
                # æ„å»ºå®Œæ•´çš„æ€è€ƒè¿‡ç¨‹ï¼ˆåŒ…å«æ‰€æœ‰ä¸­é—´æ­¥éª¤å’Œæœ€ç»ˆç»“æœï¼‰
                thinking_text = self._build_complete_thinking_text(intermediate_steps, agent_result)
                
                logger.info(f"æ€è€ƒè¿‡ç¨‹é•¿åº¦: {len(thinking_text)} å­—ç¬¦")
                # è®°å½•å®Œæ•´çš„æ€è€ƒå†…å®¹ç”¨äºè°ƒè¯•
                if len(thinking_text) > 5000:
                    logger.debug(f"æ€è€ƒè¿‡ç¨‹å†…å®¹ï¼ˆå‰2000å­—ï¼‰: {thinking_text[:2000]}")
                    logger.debug(f"æ€è€ƒè¿‡ç¨‹å†…å®¹ï¼ˆä¸­é—´2000å­—ï¼‰: {thinking_text[len(thinking_text)//2-1000:len(thinking_text)//2+1000]}")
                    logger.debug(f"æ€è€ƒè¿‡ç¨‹å†…å®¹ï¼ˆæœ«å°¾2000å­—ï¼‰: {thinking_text[-2000:]}")
                else:
                    logger.debug(f"å®Œæ•´æ€è€ƒè¿‡ç¨‹: {thinking_text}")
                
                # ä¸€æ¬¡æ€§è¾“å‡ºå®Œæ•´çš„æ€è€ƒè¿‡ç¨‹ï¼ˆä¸åˆ†å—ï¼Œç¡®ä¿å®Œæ•´ï¼‰
                yield {"type": "thinking", "content": thinking_text}
                
                # ç¬¬äºŒæ­¥: åŸºäºAgentçš„ç»“æœ,æµå¼ç”Ÿæˆæ›´å¥½çš„å›ç­”
                logger.info("Agentæ€è€ƒå®Œæˆ,å¼€å§‹æµå¼è¾“å‡ºä¼˜åŒ–åçš„ç­”æ¡ˆ")
                
                # æ„å»ºåŒ…å«Agentæ€è€ƒç»“æœçš„æç¤º
                history_messages = []
                for human_msg, ai_msg in chat_history:
                    history_messages.append(HumanMessage(content=human_msg))
                    history_messages.append(AIMessage(content=ai_msg))
                
                stream_messages = [
                    SystemMessage(content="ä½ æ˜¯ä¸€ä¸ªå‹å¥½çš„ä¸­æ–‡AIåŠ©æ‰‹ã€‚æ ¹æ®æä¾›çš„ä¿¡æ¯,ç”¨æµç•…ã€è‡ªç„¶çš„è¯­è¨€å›ç­”ç”¨æˆ·é—®é¢˜ã€‚"),
                    *history_messages,
                    HumanMessage(content=f"ç”¨æˆ·é—®é¢˜: {message}\n\næˆ‘å·²ç»è·å–åˆ°ä»¥ä¸‹ä¿¡æ¯:\n{agent_result}\n\nè¯·åŸºäºè¿™äº›ä¿¡æ¯,ç”¨æ¸…æ™°ã€å‹å¥½çš„æ–¹å¼å›ç­”ç”¨æˆ·çš„é—®é¢˜ã€‚")
                ]
                
                try:
                    async for chunk in self.llm.astream(stream_messages):
                        try:
                            if chunk.content:
                                yield {"type": "content", "content": chunk.content}
                        except Exception as chunk_error:
                            logger.error(f"å¤„ç†æµå¼å†…å®¹å—å¼‚å¸¸: {str(chunk_error)}")
                            continue
                except Exception as final_stream_error:
                    error_msg = str(final_stream_error)
                    logger.error(f"æœ€ç»ˆæµå¼è¾“å‡ºå¤±è´¥: {error_msg}")
                    
                    # æ£€æµ‹é…é¢å’Œé™æµé”™è¯¯ï¼Œä½œä¸ºæ™®é€šå†…å®¹è¿”å›
                    if "ResourceExhausted" in error_msg or "429" in error_msg:
                        if "quota" in error_msg.lower():
                            yield {"type": "content", "content": "ğŸ˜” Gemini API é…é¢å·²ç”¨å®Œã€‚å»ºè®®åˆ‡æ¢åˆ°è±†åŒ…æˆ–é€šä¹‰åƒé—®æœåŠ¡ã€‚"}
                        else:
                            yield {"type": "content", "content": "ğŸ˜” Gemini API è¯·æ±‚é¢‘ç‡è¶…é™ã€‚è¯·ç¨åå†è¯•ã€‚"}
                    elif "invalid" in error_msg.lower() and "key" in error_msg.lower():
                        yield {"type": "content", "content": "âŒ API å¯†é’¥æ— æ•ˆã€‚"}
                    else:
                        yield {"type": "content", "content": f"âŒ è¾“å‡ºå¼‚å¸¸: {error_msg[:100]}"}
                    
        except Exception as e:
            error_msg = str(e)
            logger.error(f"æµå¼èŠå¤©å¼‚å¸¸: {error_msg}")
            
            # æ£€æµ‹é…é¢å’Œé™æµé”™è¯¯ï¼Œä½œä¸ºæ™®é€šå†…å®¹è¿”å›
            if "ResourceExhausted" in error_msg or "429" in error_msg:
                if "quota" in error_msg.lower():
                    yield {"type": "content", "content": "ğŸ˜” Gemini API é…é¢å·²ç”¨å®Œã€‚è¯·ç¨åå†è¯•æˆ–åˆ‡æ¢åˆ°å…¶ä»– AI æœåŠ¡ï¼ˆè±†åŒ…/é€šä¹‰åƒé—®ï¼‰ã€‚"}
                else:
                    yield {"type": "content", "content": "ğŸ˜” Gemini API è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œå·²è¾¾åˆ°é€Ÿç‡é™åˆ¶ã€‚è¯·ç¨åå†è¯•ã€‚"}
            elif "API_KEY_INVALID" in error_msg or "invalid API key" in error_msg.lower():
                yield {"type": "content", "content": "âŒ API å¯†é’¥æ— æ•ˆã€‚è¯·æ£€æŸ¥ Gemini API å¯†é’¥é…ç½®ã€‚"}
            elif "403" in error_msg or "permission" in error_msg.lower():
                yield {"type": "content", "content": "âŒ API æƒé™ä¸è¶³ã€‚è¯·æ£€æŸ¥ API å¯†é’¥æƒé™ã€‚"}
            elif "timeout" in error_msg.lower():
                yield {"type": "content", "content": "â±ï¸ è¯·æ±‚è¶…æ—¶ã€‚è¯·ç¨åé‡è¯•ã€‚"}
            else:
                yield {"type": "content", "content": f"âŒ æœåŠ¡å¼‚å¸¸: {error_msg[:200]}"}

@lru_cache()
def get_gemini_service(
    ai_history_mapper: AiHistoryMapper = Depends(get_ai_history_mapper),
    user_mapper: UserMapper = Depends(get_user_mapper)
) -> GeminiService:
    """è·å–GeminiæœåŠ¡å•ä¾‹å®ä¾‹"""
    return GeminiService(
            ai_history_mapper, 
            user_mapper
        )
